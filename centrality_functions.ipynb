{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a84b07",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b3c720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76741d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import cobra\n",
    "from cobra.io import read_sbml_model\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b121b82",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3373,
   "id": "89756b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betweenness_centrality_log10(model_graph_df):\n",
    "    # calculate betweenness centrality\n",
    "    model_betweenness_centrality = nx.betweenness_centrality(model_graph_df, weight='weight', normalized=False)\n",
    "    \n",
    "    # strip whitespace from keys and log-transform the values\n",
    "    log10_betweenness_centrality = {key.strip(): np.log10(value + 1) for key, value in model_betweenness_centrality.items()}\n",
    "    \n",
    "    return log10_betweenness_centrality\n",
    "\n",
    "\n",
    "def get_bridging_centrality_log10(model_graph_df):\n",
    "    # calculate betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(model_graph_df, weight='weight', normalized=False)\n",
    "\n",
    "    # calculate the bridging coefficient for each node\n",
    "    bridging_coefficient = {}\n",
    "    for node in model_graph_df.nodes():\n",
    "        \n",
    "        # for directed graphs, consider successors and predecessors\n",
    "        if model_graph_df.is_directed():\n",
    "            successors = list(model_graph_df.successors(node))\n",
    "            predecessors = list(model_graph_df.predecessors(node))\n",
    "            degree_sum = sum(model_graph_df.out_degree(successor) for successor in successors) + \\\n",
    "                         sum(model_graph_df.in_degree(predecessor) for predecessor in predecessors)\n",
    "        else:\n",
    "            # for undirected graphs, consider neighbors\n",
    "            neighbors = list(model_graph_df.neighbors(node))\n",
    "            degree_sum = sum(model_graph_df.degree(neighbor) for neighbor in neighbors)\n",
    "\n",
    "        bridging_coefficient[node] = 1 / degree_sum if degree_sum > 0 else 0\n",
    "\n",
    "    # calculate bridging centrality\n",
    "    bridging_centrality = {node: betweenness[node] * bridging_coefficient[node] for node in model_graph_df.nodes()}\n",
    "\n",
    "    # strip whitespace from keys and log-transform the values\n",
    "    log10_bridging_centrality = {key.strip(): np.log10(value + 1) for key, value in bridging_centrality.items()}\n",
    "\n",
    "    return log10_bridging_centrality\n",
    "\n",
    "def get_clustering_coefficient(model_graph_df):\n",
    "    model_clustering_coefficient = nx.clustering(model_graph_df)\n",
    "\n",
    "    # strip whitespace from keys and log-transform the values\n",
    "    clustering_coefficient = {key.strip(): value for key, value in model_clustering_coefficient.items()}\n",
    "\n",
    "    return clustering_coefficient\n",
    "\n",
    "def get_degree(model_graph_df, log=False, directional=False):\n",
    "    model_degree = dict(model_graph_df.degree())\n",
    "    model_degree = {key.strip(): value for key, value in model_degree.items()}\n",
    "    \n",
    "    if (directional):\n",
    "        c=0\n",
    "    \n",
    "    # strip whitespace from keys and log-transform the values\n",
    "    if (log):\n",
    "        model_degree = {key.strip(): np.log10(value + 1) for key, value in model_degree.items()}\n",
    "    \n",
    "    return model_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3502,
   "id": "cb642932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_isolate(dic, val):\n",
    "    match_keys=[]\n",
    "    for key, value in dic.items():\n",
    "        if (value==val):\n",
    "            match_keys.append(key)\n",
    "    return match_keys\n",
    "\n",
    "def df_isolate(df, col, val):\n",
    "    return df[df[col]==val]\n",
    "\n",
    "def log_columns(df, cols, base=2):\n",
    "    for col in cols:\n",
    "        if (base==2):\n",
    "            df[f\"log_{col}\"]=np.log(df[col]+1)\n",
    "        elif (base==10):\n",
    "            df[f\"log_{col}\"]=np.log10(df[col]+1)\n",
    "        else:\n",
    "            print(\"Choose valid base (2 or 10)\")\n",
    "    return df\n",
    "\n",
    "from sklearn import preprocessing\n",
    "def normalize(data, lower, upper):\n",
    "    data_norm=[lower + (upper - lower) * x for x in data]\n",
    "    return data_norm\n",
    "\n",
    "def adj_columns(df, cols, prefix):\n",
    "    for col in cols:\n",
    "        d=np.array(df[col])\n",
    "        df[f\"{prefix}_{col}\"]=preprocessing.normalize([d]).tolist()[0]\n",
    "    return df\n",
    "\n",
    "def z_score(df, cols, prefix):\n",
    "    for col in cols:\n",
    "        d=np.array(df[col])\n",
    "        df[f\"{prefix}_{col}\"]=stats.zscore(d).tolist()\n",
    "    return df\n",
    "\n",
    "def scale(df, cols, lb=0, ub=1):\n",
    "    for col in cols:\n",
    "        df[f\"scale_{col}\"]=scale_list(list(df[col]), lb, ub)\n",
    "    return df\n",
    "\n",
    "def scale_number(unscaled, to_min, to_max, from_min, from_max):\n",
    "    return (to_max-to_min)*(unscaled-from_min)/(from_max-from_min)+to_min\n",
    "\n",
    "def scale_list(l, to_min, to_max):\n",
    "    return [scale_number(i, to_min, to_max, min(l), max(l)) for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3503,
   "id": "611640b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_index(df, replace_pairs):\n",
    "    for r1, r2 in replace_pairs:\n",
    "        df.index=df.index.str.replace(r1,r2)\n",
    "    return df\n",
    "\n",
    "def replace_column(df, replace_pairs):\n",
    "    for r1, r2 in replace_pairs:\n",
    "        df.columns=df.columns.str.replace(r1,r2)\n",
    "    return df\n",
    "\n",
    "def sort_dict(d):\n",
    "    return {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
    "\n",
    "def sorted_counter(data):\n",
    "    return sort_dict(Counter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3504,
   "id": "11c88598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE BASED ON UPDATES MADE BELOW\n",
    "# assign bins for histogram\n",
    "def assign_bins(df, col, bins, return_hist=False):\n",
    "    col_data=list(df[col])\n",
    "    # make sure min of data is not zero\n",
    "    if (min(col_data)==0):\n",
    "        print(\"Warning, minimum of data is zero.\")\n",
    "    hist,bin_edges=np.histogram(col_data, bins=bins)\n",
    "    hist=list(hist)\n",
    "    bin_edges=list(bin_edges)\n",
    "    \n",
    "    # not needed\n",
    "#     bin_edges.insert(0, 0)\n",
    "#     max_p1=max(col_data)+1\n",
    "#     bin_edges.append(max_p1)\n",
    "    \n",
    "    # get bin edge pairs\n",
    "    bin_edge_pairs={}\n",
    "    for i in range(len(bin_edges)):\n",
    "        if (i>=len(bin_edges)-1):\n",
    "            break\n",
    "        pair=(bin_edges[i], bin_edges[i+1])\n",
    "        bin_edge_pairs[pair]=f\"g{i}\"        \n",
    "        \n",
    "    # bound the data\n",
    "    df_hist=pd.DataFrame()\n",
    "    for bin_pair, label in zip(bin_edge_pairs.keys(), bin_edge_pairs.values()):\n",
    "        df_bounded=df.loc[df[col].between(bin_pair[0], bin_pair[1])]\n",
    "        df_bounded[\"hist_label\"]=label\n",
    "        df_hist=pd.concat([df_hist, df_bounded])\n",
    "        \n",
    "    if (return_hist):\n",
    "        return df_hist, hist, bin_edges, bin_edge_pairs\n",
    "    else:\n",
    "        return df_hist\n",
    "    \n",
    "# computes fraction of essential reactions\n",
    "def compute_fraction_essential_hist(df_hist, hist, bin_edges, bin_edge_pairs, compute_ratio=False):\n",
    "    df_ess=pd.DataFrame()\n",
    "    df_ess[\"hist_label\"]=list(bin_edge_pairs.values())\n",
    "    df_ess[\"hist_counts\"]=hist\n",
    "    ess_counts=[]\n",
    "    noness_counts=[]\n",
    "    ess_fracs=[]\n",
    "    bin_centers=[]\n",
    "    left_bin=[]\n",
    "    right_bin=[]\n",
    "    for pair, label in zip(bin_edge_pairs.keys(), bin_edge_pairs.values()):\n",
    "        bin_centers.append(np.mean(pair))\n",
    "        left_bin.append(pair[0])\n",
    "        right_bin.append(pair[1])\n",
    "        df_hist_label=df_hist[df_hist[\"hist_label\"]==label]\n",
    "        num_ess=len(df_hist_label[df_hist_label[\"ess\"]==1])\n",
    "        num_noness=len(df_hist_label[df_hist_label[\"ess\"]==0])\n",
    "        if (compute_ratio):\n",
    "            if (num_noness==0):\n",
    "                frac_ess=0\n",
    "            else:\n",
    "                frac_ess=num_ess/num_noness\n",
    "        else:\n",
    "            if ((num_ess+num_noness)==0):\n",
    "                frac_ess=0\n",
    "            else:\n",
    "                frac_ess=num_ess/(num_ess+num_noness)\n",
    "        \n",
    "        # append data\n",
    "        ess_counts.append(num_ess)\n",
    "        noness_counts.append(num_noness)\n",
    "        ess_fracs.append(frac_ess)\n",
    "        \n",
    "    # update df\n",
    "    df_ess[\"num_ess\"]=ess_counts\n",
    "    df_ess[\"num_noness\"]=noness_counts\n",
    "    df_ess[\"frac_ess\"]=ess_fracs\n",
    "    df_ess[\"bin_centers\"]=bin_centers\n",
    "    df_ess[\"left_bin\"]=left_bin\n",
    "    df_ess[\"right_bin\"]=right_bin\n",
    "    \n",
    "    # return \n",
    "    return df_ess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cfa481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts reactant and product sharing across all reactions\n",
    "def all_prods_reacts_dict(fba_ecoli_lofo, ecoli_model, rev=False):\n",
    "    \n",
    "    all_reactants=[]\n",
    "    all_products=[]\n",
    "    for rxn in list(fba_ecoli_lofo.keys()):\n",
    "        rxn_df=ecoli_model.reactions.get_by_id(rxn)\n",
    "        rxn_reactants=rxn_df.reactants\n",
    "        rxn_products=rxn_df.products\n",
    "        rxn_reactants=[i.id for i in rxn_reactants]\n",
    "        rxn_products=[i.id for i in rxn_products]\n",
    "\n",
    "        rxn_reversibility=rxn_df.reversibility\n",
    "        if (rev):\n",
    "            if (rxn_reversibility):\n",
    "                all_mets=rxn_reactants+rxn_products\n",
    "                all_reactants+=all_mets\n",
    "                all_products+=all_mets\n",
    "            else:\n",
    "                all_reactants+=rxn_reactants\n",
    "                all_products+=rxn_products\n",
    "        else:\n",
    "            all_reactants+=rxn_reactants\n",
    "            all_products+=rxn_products\n",
    "        \n",
    "    # count and convert to dictionary\n",
    "    all_products_dict=Counter(all_products)\n",
    "    all_reactants_dict=Counter(all_reactants)\n",
    "\n",
    "    # sort dictionaries and create a list of metabolites to ignore because they are too prevelant\n",
    "    all_products_dict={k: v for k, v in sorted(all_products_dict.items(), key=lambda item: item[1])}\n",
    "    all_reactants_dict={k: v for k, v in sorted(all_reactants_dict.items(), key=lambda item: item[1])}\n",
    "        \n",
    "    return all_reactants_dict, all_products_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e6756fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reactants_dict, all_products_dict=all_prods_reacts_dict(fba_ecoli_lofo, ecoli_model, rev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39b0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stats.ttest_ind(list(all_reactants_ess_dict.values()), list(all_reactants_noness_dict.values()), equal_var=False))\n",
    "# print(stats.ttest_ind(list(all_products_ess_dict.values()), list(all_products_noness_dict.values()), equal_var=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8b1016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count and convert to dictionary\n",
    "# all_products_dict=Counter(all_products)\n",
    "# all_reactants_dict=Counter(all_reactants)\n",
    "\n",
    "# # sort dictionaries and create a list of metabolites to ignore because they are too prevelant\n",
    "# all_products_dict={k: v for k, v in sorted(all_products_dict.items(), key=lambda item: item[1])}\n",
    "# all_reactants_dict={k: v for k, v in sorted(all_reactants_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "id": "4b725bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe info in one dict\n",
    "def split_react_prod_dict_by_ess(fba_ecoli_lofo):\n",
    "    all_reactants_ess=[]\n",
    "    all_products_ess=[]\n",
    "    all_reactants_noness=[]\n",
    "    all_products_noness=[]\n",
    "\n",
    "    for rxn, obj in zip(fba_ecoli_lofo.keys(), fba_ecoli_lofo.values()):\n",
    "        rxn_df=ecoli_model.reactions.get_by_id(rxn)\n",
    "        rxn_reactants=rxn_df.reactants\n",
    "        rxn_products=rxn_df.products\n",
    "        rxn_reactants=[i.id for i in rxn_reactants]\n",
    "        rxn_products=[i.id for i in rxn_products]\n",
    "        if (obj < 2):\n",
    "            all_reactants_ess+=rxn_reactants\n",
    "            all_products_ess+=rxn_products\n",
    "        else:\n",
    "            all_reactants_noness+=rxn_reactants\n",
    "            all_products_noness+=rxn_products\n",
    "\n",
    "    # create dicts\n",
    "    all_reactants_ess_dict=dict(Counter(all_reactants_ess))\n",
    "    all_products_ess_dict=dict(Counter(all_products_ess))\n",
    "    all_reactants_noness_dict=dict(Counter(all_reactants_noness))\n",
    "    all_products_noness_dict=dict(Counter(all_products_noness))\n",
    "    \n",
    "    return all_reactants_ess_dict, all_products_ess_dict, all_reactants_noness_dict, all_products_noness_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e926bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'centrality_functions.py'; 'centrality_functions' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34180/4019953563.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcentrality_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdegree_centrality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'centrality_functions.py'; 'centrality_functions' is not a package"
     ]
    }
   ],
   "source": [
    "from centrality_functions.py import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb5603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
